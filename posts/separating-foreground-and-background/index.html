<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><style>:root{--accent-color:#E8888A}</style><title>Separating the Foreground and Background of a Video</title><meta name=description content="Random articles and thoughts"><meta name=keywords content="blog,gokarna,hugo,math,university"><meta property="og:url" content="https://filipre.github.io/posts/separating-foreground-and-background/"><meta property="og:type" content="website"><meta property="og:title" content="Separating the Foreground and Background of a Video"><meta property="og:description" content="Random articles and thoughts"><meta property="og:image" content="https://filipre.github.io/images/avatar.png"><meta property="og:image:secure_url" content="https://filipre.github.io/images/avatar.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="Separating the Foreground and Background of a Video"><meta name=twitter:description content="Random articles and thoughts"><meta property="twitter:domain" content="https://filipre.github.io/posts/separating-foreground-and-background/"><meta property="twitter:url" content="https://filipre.github.io/posts/separating-foreground-and-background/"><meta name=twitter:image content="https://filipre.github.io/images/avatar.png"><link rel=canonical href=https://filipre.github.io/posts/separating-foreground-and-background/><link rel=stylesheet type=text/css href=/css/normalize.min.css media=print><link rel=stylesheet type=text/css href=/css/main.min.css><link id=dark-theme rel=stylesheet href=/css/dark.min.css><script src=/js/bundle.min.edd985581bf860dfb4507e5885197f1680160c7fe19f23b31e183126d99dd596.js integrity="sha256-7dmFWBv4YN+0UH5YhRl/FoAWDH/hnyOzHhgxJtmd1ZY="></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})})</script></head><body><script type=text/javascript>setThemeByUserPref()</script><header class=header><nav class=header-nav><div class=avatar><a href=https://filipre.github.io/><img src=/images/avatar.png alt=avatar></a></div><div class=nav-title><a class=nav-brand href=https://filipre.github.io/>Ren√©'s Blog</a></div><div class=nav-links><div class=nav-link><a href=https://filipre.github.io/posts/><span data-feather=book></span> Posts</a></div><div class=nav-link><a href=https://filipre.github.io/tags/><span data-feather=tag></span> Tags</a></div><div class=nav-link><a href=https://filipre.github.io/contact><span data-feather=mail></span> Contact</a></div><span class=nav-icons-divider></span><div class="nav-link dark-theme-toggle"><span id=dark-theme-toggle-screen-reader-target class=sr-only></span>
<a><span id=theme-toggle-icon data-feather=moon></span></a></div><div class=nav-link id=hamburger-menu-toggle><span id=hamburger-menu-toggle-screen-reader-target class=sr-only>menu</span>
<a><span data-feather=menu></span></a></div><ul class="nav-hamburger-list visibility-hidden"><li class=nav-item><a href=https://filipre.github.io/posts/><span data-feather=book></span> Posts</a></li><li class=nav-item><a href=https://filipre.github.io/tags/><span data-feather=tag></span> Tags</a></li><li class=nav-item><a href=https://filipre.github.io/contact><span data-feather=mail></span> Contact</a></li><li class="nav-item dark-theme-toggle"><span id=dark-theme-toggle-screen-reader-target class=sr-only>theme</span>
<a><span id=theme-toggle-icon data-feather=moon></span></a></li></ul></div></nav></header><main id=content><div class="post container"><div class=post-header-section><h1>Separating the Foreground and Background of a Video</h1><small role=doc-subtitle></small><p class=post-date>17 March, 2019</p><ul class=post-tags><li class=post-tag><a href=https://filipre.github.io/tags/math>math</a></li><li class=post-tag><a href=https://filipre.github.io/tags/university>university</a></li></ul></div><div class=post-content><p><p>Currently, I am taking a class called &ldquo;Convex Optimization for Machine Learning & Computer Vision&rdquo;. Even though the lectures are quite theoretical, the programming homework are not and we do some interesting projects like this one: How would you separate the background and the foreground of a video using optimization?</p><p><img src=/posts/separating-foreground-and-background/background-foreground.gif alt="background and foreground of a video"></p><p>Most likely, you would somehow make the assumption that the background takes most of the space and stays still while the foreground are all the smaller parts that are moving. But how would we describe that mathematically and how would we &ldquo;implement&rdquo; this assumption? That&rsquo;s what we gonna cover in this blog post.</p><h2 id=basic-idea>Basic Idea</h2><p>We are given a video and we want to separate the background $A$ from the foreground $B$. We can think of each original frame as a matrix in $\mathbb{R}^{w \times h}$. In order to work with the data, we vectorize each frame and then stack everything together to form a matrix $Z \in \mathbb{R}^{n_1 \times n_2}$ that contains the whole video at once where $n_1 = w \cdot h$ and $n_2$ are the number of frames. Thus, we try to find the background matrix $A \in \mathbb{R}^{n_1 \times n_2}$ and the foreground matrix $B \in \mathbb{R}^{n_1 \times n_2}$ such that $Z = A + B$.</p><p>Let&rsquo;s suppose we have a function $f_\mathrm{change}$ that measures how much a matrix (i.e. the video) &ldquo;changes&rdquo; and we have a second function $f_\mathrm{size}$ that measures how many non-zero elements a matrix has (i.e. how much &ldquo;space&rdquo; an animation might take in comparison to the whole video). Then, we can formulate following constrained optimization problem:</p><p>$$\argmin_{A, B} f_\mathrm{change}(A) + \lambda f_\mathrm{size}(B) \ \text{s.t.} \ A + B = Z$$</p><p>The $\lambda \in \mathbb{R}$ sort of weights how much emphasis we want to put into the two objectives. As it turns out, it also makes sense to weaken our constraint a bit. Instead of reconstructing the <em>exact</em> image we could also reconstruct a <em>similar</em> image $M$ such that the difference between the two images are below a certain threshold $\epsilon$</p><p>$$\argmin_{A, B, M} f_\mathrm{change}(A) + \lambda f_\mathrm{size}(B) \ \text{s.t.} \ A+B=M \ \text{and} \ \lVert M - Z\rVert_{\mathrm{fro}} \le \epsilon$$</p><p>Before we take a closer look at the two mysterious functions $f_\mathrm{change}$ and $f_\mathrm{size}$ we transform the constrained optimization problem into an unconstrained one using the indicator function $\delta$ that is 0 if the argument is true and $+\infty$ otherwise. This enables us to use methods from convex optimization.</p><p>$$\argmin_{A, B, M} f_\mathrm{change}(A) + \lambda f_\mathrm{size}(B) + \delta{A+B-M=0} + \delta{\lVert M - Z\rVert_{\mathrm{fro}} \le \epsilon}$$</p><h2 id=vector-p-norm>Vector $p$-Norm</h2><p>Most of the norms you usually encounter are vector $p$-norms. For $p \ge 1$ they are defined like this</p><p>$$\lVert x \rVert_p = \left( \sum^n_{i=1} |x_i|^p \right)^{1/p}$$</p><p>For example, if you take $p=2$ you get the Euclidean norm that can measure the &ldquo;direct&rdquo; distance between two vectors. Or, you take $p=1$ and you get the Manhattan norm that can measure the distance when you are only allowed to move on a grid (like it is in Manhattan).</p><p>For any $p \ge 1$ we have a <em>true</em> norm and any norm turns out to be convex. Proof: Let $x, y \in \mathrm{E}$ and $0 \le \alpha \le 1$. A function $F$ is convex iff $F(\alpha x + (1-\alpha) y) \le \alpha F(x) + (1-\alpha) F(y)$.</p><p>$$\begin{aligned}
\lVert \cdot \rVert(\alpha x + (1 - \alpha) y) &\le \lVert \cdot \rVert(\alpha x) + \lVert \cdot \rVert((1 - \alpha) y) \
&= \alpha \lVert \cdot \rVert(x) + (1-\alpha) \lVert \cdot \rVert(y)
\end{aligned}$$</p><p>However, for $0 &lt; p &lt; 1$ the $p$-norm is not a norm but a <a href=https://en.wikipedia.org/wiki/Quasinorm>Quasinorm</a> and therefore, the function is not necessarily convex anymore. You can also see in the visualization that the epigraph (the level curves) are not a convex set.</p><p>Notice how the edges at the axes become sharper and sharper. Taking $p=0$ breaks the original $p$-norm formula (whats the 0-th root of a number?) so people agreed to define the 0-norm as the number of non-zero elements in a vector</p><p>$$\lVert x \rVert_0 = |{ x_i \ \colon \ x_i \ne 0 }|$$</p><p>The 0-norm looks like a suitable candidate for our $f_\mathrm{size}$ function. We assume that the foreground takes only a small part of the video, i.e. each frame will have many zero elements in it and our foreground matrix $B$ will be <a href=https://en.wikipedia.org/wiki/Sparse_matrix>sparse</a>. Thus, we want to minimize $B$ over the 0-norm. Sadly, the 0-norm is not only non-convex but also too hard to optimize. We do know how to optimize over convex functions so we need to find a suitable convex alternative to the 0-norm that behaves not the same but close enough.</p><p>Let&rsquo;s suppose we are in $R^2$ only. The blue line below shows all optimal $(x_1, x_2)$-pair solutions for any given problem. So which solution should we pick? Clearly there are infinite many. We could make the additional requirement to make the solution as sparse as possible, i.e. either $x_1$ or $x_2$ should be zero. It should be obvious that the solution will be the intersection of the blue line with the $x_1$- or $x_2$-axis. But how would we get the intersection? Instead of optimizing over the original energy function (the blue line) only, we will also optimize over the 1-norm (red line), i.e. we will make the area of the red square as small as possible such that the red edges still hit any part of the blue line.</p><p><img src=/posts/separating-foreground-and-background/l1_energy.png alt="l1 optimization"></p><p>Because the 1-norm has &ldquo;edges&rdquo; at the axes, it is likely that we end up with a sparse solution. This is also known as &ldquo;l1-norm regularization&rdquo; and you can read a nice blog post <a href=https://medium.com/mlreview/l1-norm-regularization-and-sparsity-explained-for-dummies-5b0e4be3938a>here</a> that explains this trick more in detail. For $p > 1$ we lose the edges at the axes and for $p &lt; 1$ our &ldquo;norm&rdquo; wouldn&rsquo;t be convex anymore, so the 1-norm seems to be a reasonable candidate for enforcing sparsity on the video matrix $B$.</p><p>One tiny part is missing: $B$ is a matrix and the 1-norm is defined for vectors. Thus, we need to vectorize $B$ first before we apply the 1-norm. There is sometimes an abuse of notation, and the 1-norm of a matrix refers to</p><p>$$\lVert B \rVert_1 = \lVert\operatorname{vec} B\rVert_1$$</p><h2 id=matrix-schatten-p-norm>Matrix Schatten $p$-Norm</h2><p>There are several ways to define a matrix norm and things can become confusing. One way is by vectorizing the matrix and actually referring to a vector norm like it was the case above. A general formula would be</p><p>$$\lVert A \rVert_p = \left( \sum_{i=1}^m \sum_{j=1}^n | a_{ij} |^p \right)^{1/p} = \lVert \operatorname{vec} A \rVert_p$$</p><p>One famous special case $p=2$ is the Frobenius norm $\lVert \cdot \rVert_\mathrm{fro}$ that matches the definition of the Euclidean norm.</p><p>Sometimes, a matrix norm refers to the <a href=https://en.wikipedia.org/wiki/Operator_norm>operator norm</a> which &ldquo;measure the &lsquo;size&rsquo; of certain linear operators&rdquo;. Let $A$ be a linear mapping from $K^m$ to $K^n$. Then we could define $\lVert A \rVert_p$ as</p><p>$$\lVert A \rVert_p = \sup_{x \ne 0} \frac{\lVert Ax \rVert_p}{\lVert x \rVert_p}$$</p><p>Or, we could apply a <a href=https://en.wikipedia.org/wiki/Singular_value_decomposition>Singular value decomposition</a> (SVD) on $A$ and then define a norm using the singular values $\sigma_1, \ldots, \sigma_r$. That is the idea of the Schatten $p$-norm and that is what we gonna look more in detail now.</p><p>$$\lVert A \rVert_p = \left( \sum_{i=1}^r \sigma_i^p \right)^{1/p}$$</p><p>To understand the SVD we first need to understand the <a href>rank</a> of a matrix. mathematically, it tells you how many rows (or columns) are linearly independent, but I like to think of it as &ldquo;how interesting&rdquo; your matrix is. Suppose you take $n$ measurements and your data is $d$ dimensional. Then, you could represent your data as an $n \times d$ matrix. Now, if your measurements are all similar or you could find some simple (linear) &ldquo;rule&rdquo; that can explain all rows (or columns) your matrix has, then the matrix is probably low rank and you could throw away a lot of unnecessary data. In contranst, if it turns out that each measurement has some interesting information which is not present in the other measurements, then you better keep it in order not to lose information. In some sense, the rank also indicates, how much you could compress your data without losing meaningful insights.</p><p>One technique to come up with &ldquo;rules&rdquo; that explain your data in a more compressed way is to calculate the singular value decomposition.</p><p>$$A = U \Sigma V^T$$</p><p>where $U$ and $V$ are orthogonal matrices ($U^TU = UU^T = I$) and $\Sigma$ is a diagonal matrix consisting of $r = \operatorname{rank} A$ singular values $\sigma_1 \ge \cdots \ge \sigma_r$ unequal to 0 and $\sigma_{r+1} = \cdots = \sigma_n$ equal to zero. $U$ consists of the necessary $r$ orthonormal bases and $\Sigma$ tells you how much you have to scale them to reconstruct entries in $A$.</p><p>The higher a specific singluar value is, the more important it is to explain the data. For example, if your data follows a linear trend in $\mathbb{R}^2$, then you will end up with a very high first singular value and a quite small second singular value. And if you can explain your data <em>perfectly</em> using a simple line, then the second singular value will be zero and your rank is 1 because</p><p>$$\operatorname{rank} A = | { \sigma_i \ \colon \ \sigma_i \ne 0, A = U \Sigma V^T } | = |{ \sigma_1 }| = 1$$</p><p>Now, remember that the Schatten $p$-norm is defined over the singular values for $1 \le p \le +\infty$. Interestingly enough, taking $p = 2$ also yields the Frobenius norm.</p><p>Setting $p=0$ will not give us a real norm as it was the case with the vector 0-norm and it won&rsquo;t be convex anymore but it turns out that it counts the non-zero elements of your singular values. Do you see the similarity between the vector 0-norm and matrix Schatten 0-norm?</p><p>$$\lVert A \rVert_0 = |{ \sigma_i \ \colon \ \sigma_i \ne 0 }| = \operatorname{rank} A$$</p><p>Going back to our video separation problem, we assumed that the background of the video does not change and is probably the same frame for the whole video, i.e. all columns of $A$ (frames) should be more or less the same image vector. If we can minimize the rank of $A$ we would archive this. But again, the rank is not convex and we need to find a convex substitute.</p><p>Like we did in the previous section, we will replace the Schatten 0-norm with the Schatten 1-norm, which is also known as the nuclear norm. This is a <a href>good</a> approximation to the rank that we can work with.</p><p>$$\lVert A \rVert_\mathrm{nuc} = \sum_i^r \sigma_i$$</p><p>So, our final optimization problem looks like this</p><p>$$\argmin_{A, B, M} \lVert A \rVert_\mathrm{nuc} + \lambda \lVert B \rVert_1 + \delta{A+B-M=0} + \delta{\lVert M - Z\rVert_{\mathrm{fro}} \le \epsilon}$$</p><p>Notice that none of the terms are differentiable, so simple gradient descent or <a href>forward backward splitting</a> does not work here.</p><h2 id=alternating-direction-method-of-multipliers-admm>Alternating Direction Method of Multipliers (ADMM)</h2><p>Instead, we use ADMM. For now, I won&rsquo;t go into details how to derive these update rules but the idea is to introduce a Lagrangian Multiplier $Y$ to enforce $A+B=M$ and optimize over $A$, $B$, $M$ and finally $Y$ in an alternating way. Each optimization problem can be reformulated using the $\operatorname{prox}$ operator and the $\operatorname{prox}$ of the Nuclear norm, the L1 norm and the indicator function are well known. Please have a look at the Python code to see the update iteration. We reformulate the problem as the Augmented Lagrangian.</p><p>$$\begin{aligned}
\min_{A, B, M} \max_{Y} \mathcal{L}(A, B, M; Y)
= &\lVert A \rVert_\mathrm{nuc}</p><ul><li>\lVert B \rVert_1</li><li>\delta{\lVert M - Z\rVert_{\mathrm{fro}} \le \epsilon} \
&+ \langle A+B-M, Y \rangle</li><li>\frac{\rho}{2} \lVert A+B^k-M^k \rVert_\mathrm{fro}^2
\end{aligned}$$</li></ul><h3 id=optimization-over-a-low-rank-matrix>Optimization over A: Low Rank Matrix</h3><p>$$\begin{aligned}
A^{k+1} &\in \argmin_{A} \lVert A \rVert_\mathrm{nuc} + \langle Y^k, A \rangle + \frac{\rho}{2} \lVert A+B^k-M^k \rVert_\mathrm{fro}^2 \
&= \argmin_{A} \lVert A \rVert_\mathrm{nuc} + \frac{\rho}{2} ( 2 \langle \tfrac{1}{\rho} Y^k, A \rangle + \langle A+B^k-M^k, A+B^k-M^k \rangle ) \
&= \argmin_{A} \lVert A \rVert_\mathrm{nuc} + \frac{\rho}{2} \langle A+B-M^k+\tfrac{1}{\rho}Y^k, A+B-M^k+\tfrac{1}{\rho}Y^k \rangle \
&= \argmin_{A} \lVert A \rVert_\mathrm{nuc} + \frac{\rho}{2} \lVert A - (M^k - B^k - \tfrac{1}{\rho} Y^k) \rVert_\mathrm{fro}^2 \
&= \operatorname{prox}<em>{\lVert \cdot \rVert</em>\mathrm{nuc} / \rho}(\underbrace{M^k - B^k - \tfrac{1}{\rho} Y^k}<em>X ) \
&= U \operatorname{diag} ( { (\sigma_i - \tfrac{1}{\rho})</em>+ } ) V^T \ \text{where} \ X = U \Sigma V^T
\end{aligned}$$</p><h3 id=optimization-over-b-sparse-matrix>Optimization over B: Sparse Matrix</h3><p>$$\begin{aligned}
B^{k+1} &\in \argmin_{B} \lambda \lVert B \rVert_1 + \langle Y^k, B \rangle + \frac{\rho}{2} \lVert A^{k+1}+B-M^k \rVert_\mathrm{fro}^2 \
&= \operatorname{prox}<em>{\lVert \cdot \rVert_1 \lambda / \rho} (M^k - A^{k+1} - \tfrac{1}{\rho} Y^k) \
\operatorname{vec} B^{k+1} &= \operatorname{prox}</em>{\lVert \operatorname{vec}(\cdot) \rVert_1 \lambda / \rho} ( \underbrace{\operatorname{vec} (M^k - A^{k+1} - \tfrac{1}{\rho} Y^k)}_{x}) \
&= b \in \mathbb{R}^{n_1 n_2} \ \colon \ b_i =
\begin{cases}
x_i + \tfrac{\lambda}{\rho} &\text{if } x_i &lt; -\tfrac{\lambda}{\rho} \
0 &\text{if } x_i \in [ -\tfrac{\lambda}{\rho}, \tfrac{\lambda}{\rho} ] \
x_i - \tfrac{\lambda}{\rho} &\text{if } x_i > \tfrac{\lambda}{\rho}
\end{cases}
\end{aligned}$$</p><h3 id=optimization-over-m-reconstruction>Optimization over M: Reconstruction</h3><p>$$\begin{aligned}
M^{k+1} &\in \argmin_M \delta { \lVert M - Z \rVert_\mathrm{fro} \le \epsilon } - \langle Y^k, M \rangle + \frac{\rho}{2} \lVert A^{k+1} + B^{k+1} - M\rVert_\mathrm{fro}^2 \
&= \operatorname{prox}<em>{\delta { \lVert M - Z \rVert</em>\mathrm{fro} \le \epsilon } / \rho } (\underbrace{A^{k+1} + B^{k+1} + \tfrac{1}{\rho} Y^k}<em>X) \
&= \operatorname{proj}</em>{C} (X) \ \text{and} \ C={M \in \mathbb{R}^{n_1 \times n_2} \ \colon \ \lVert M - Z \rVert_\mathrm{fro} \le \epsilon} \
\operatorname{vec} M^{k+1} &= \operatorname{proj}_{C&rsquo;} (\operatorname{vec} X) \ \text{and} \ C&rsquo;= { m \in \mathbb{R}^{n_1 n_2} \ \colon \ \lVert m - \operatorname{vec}Z \rVert_2 \le \epsilon} = \overline{B}(\operatorname{vec}{Z}, \epsilon)\
&= \operatorname{vec}Z + \frac{\epsilon}{\max{ \lVert \operatorname{vec}X - \operatorname{vec}Z \rVert_2, \epsilon }} (\operatorname{vec} X - \operatorname{vec} Z)
\end{aligned}$$</p><h3 id=dual-ascend-step-for-y>Dual Ascend Step for Y</h3><p>In ADMM, we perform a gradient ascend step for the dual variable $Y$ to enforce the equality constraint:</p><p>$$Y^{k+1} = Y^k + \rho * (A^{k+1} + B^{k+1} - M^{k+1})$$</p><h2 id=implementation-using-numpy>Implementation using Numpy</h2><p>Update Rules</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>calcA</span>(B, M, Y, rho):
</span></span><span style=display:flex><span>    X <span style=color:#f92672>=</span> M <span style=color:#f92672>-</span> B <span style=color:#f92672>-</span> (<span style=color:#ae81ff>1</span><span style=color:#f92672>/</span>rho)<span style=color:#f92672>*</span>Y
</span></span><span style=display:flex><span>    n1, n2 <span style=color:#f92672>=</span> X<span style=color:#f92672>.</span>shape
</span></span><span style=display:flex><span>    U, S, VH <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>linalg<span style=color:#f92672>.</span>svd(X, full_matrices<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>    diag_plus <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>diag(np<span style=color:#f92672>.</span>maximum(np<span style=color:#f92672>.</span>zeros(S<span style=color:#f92672>.</span>shape), S <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span><span style=color:#f92672>/</span>rho))
</span></span><span style=display:flex><span>    A <span style=color:#f92672>=</span> U <span style=color:#f92672>@</span> diag_plus <span style=color:#f92672>@</span> VH
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> A
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>calcB</span>(A, M, Y, rho, lamb):
</span></span><span style=display:flex><span>    X <span style=color:#f92672>=</span> M <span style=color:#f92672>-</span> A <span style=color:#f92672>-</span> (<span style=color:#ae81ff>1</span><span style=color:#f92672>/</span>rho)<span style=color:#f92672>*</span>Y
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> mat2vec(X)
</span></span><span style=display:flex><span>    lamb_rho <span style=color:#f92672>=</span> lamb <span style=color:#f92672>/</span> rho
</span></span><span style=display:flex><span>    b <span style=color:#f92672>=</span> x
</span></span><span style=display:flex><span>    b[np<span style=color:#f92672>.</span>where(b <span style=color:#f92672>&lt;</span> <span style=color:#f92672>-</span>lamb_rho)] <span style=color:#f92672>+=</span> lamb_rho
</span></span><span style=display:flex><span>    b[np<span style=color:#f92672>.</span>where(b <span style=color:#f92672>&gt;</span> lamb_rho)] <span style=color:#f92672>-=</span> lamb_rho
</span></span><span style=display:flex><span>    B <span style=color:#f92672>=</span> vec2mat(b, X<span style=color:#f92672>.</span>shape)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> B
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>calcM</span>(A, B, Y, Z, rho, eps):
</span></span><span style=display:flex><span>    X <span style=color:#f92672>=</span> A <span style=color:#f92672>+</span> B <span style=color:#f92672>+</span> (<span style=color:#ae81ff>1</span><span style=color:#f92672>/</span>rho)<span style=color:#f92672>*</span>Y
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> mat2vec(X)
</span></span><span style=display:flex><span>    z <span style=color:#f92672>=</span> mat2vec(Z)
</span></span><span style=display:flex><span>    m <span style=color:#f92672>=</span> z <span style=color:#f92672>+</span> eps <span style=color:#f92672>/</span> (np<span style=color:#f92672>.</span>maximum(np<span style=color:#f92672>.</span>linalg<span style=color:#f92672>.</span>norm(x<span style=color:#f92672>-</span>z), eps)) <span style=color:#f92672>*</span> (x<span style=color:#f92672>-</span>z)
</span></span><span style=display:flex><span>    M <span style=color:#f92672>=</span> vec2mat(m, X<span style=color:#f92672>.</span>shape)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> M
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>calcY</span>(A, B, M, Y, rho):
</span></span><span style=display:flex><span>    Y <span style=color:#f92672>=</span> Y <span style=color:#f92672>+</span> rho <span style=color:#f92672>*</span> (A <span style=color:#f92672>+</span> B <span style=color:#f92672>-</span> M)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> Y
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>calcEnergy</span>(A, B, M, Y, rho, lamb):
</span></span><span style=display:flex><span>    A_nuc <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>linalg<span style=color:#f92672>.</span>norm(A, ord<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;nuc&#39;</span>)
</span></span><span style=display:flex><span>    B_1 <span style=color:#f92672>=</span> lamb<span style=color:#f92672>*</span>np<span style=color:#f92672>.</span>linalg<span style=color:#f92672>.</span>norm(mat2vec(B), ord<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    inner <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>trace(Y<span style=color:#f92672>.</span>T<span style=color:#f92672>.</span>dot(A<span style=color:#f92672>+</span>B<span style=color:#f92672>-</span>M))
</span></span><span style=display:flex><span>    fro <span style=color:#f92672>=</span> rho<span style=color:#f92672>/</span><span style=color:#ae81ff>2</span> <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>linalg<span style=color:#f92672>.</span>norm(A<span style=color:#f92672>+</span>B<span style=color:#f92672>-</span>M)
</span></span><span style=display:flex><span>    energy <span style=color:#f92672>=</span> A_nuc <span style=color:#f92672>+</span> B_1 <span style=color:#f92672>+</span> inner <span style=color:#f92672>+</span> fro
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> energy
</span></span></code></pre></div><p>Update Iteration</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>for</span> it <span style=color:#f92672>in</span> range(max_it<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>):
</span></span><span style=display:flex><span>    <span style=color:#75715e># update A:</span>
</span></span><span style=display:flex><span>    A <span style=color:#f92672>=</span> calcA(B, M, Y, rho)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># update B:</span>
</span></span><span style=display:flex><span>    B <span style=color:#f92672>=</span> calcB(A, M, Y, rho, lamb)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># update M:</span>
</span></span><span style=display:flex><span>    M <span style=color:#f92672>=</span> calcM(A, B, Y, Z, rho, eps)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># update Y:</span>
</span></span><span style=display:flex><span>    Y <span style=color:#f92672>=</span> calcY(A, B, M, Y, rho)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># update augmented lagrangian energies</span>
</span></span><span style=display:flex><span>    energy <span style=color:#f92672>=</span> calcEnergy(A, B, M, Y, rho, lamb)
</span></span><span style=display:flex><span>    energies<span style=color:#f92672>.</span>append(energy)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># output status</span>
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;Iteration:&#34;</span>, it, <span style=color:#e6db74>&#34;/&#34;</span>, max_it, <span style=color:#e6db74>&#34;, Energy:&#34;</span>, energy)
</span></span></code></pre></div></p></div><div class=prev-next></div><svg id="btt-button" class="arrow-logo" xmlns="http://www.w3.org/2000/svg" height="1em" viewBox="0 0 384 512" onclick="topFunction()" title="Go to top"><path d="M177 159.7l136 136c9.4 9.4 9.4 24.6.0 33.9l-22.6 22.6c-9.4 9.4-24.6 9.4-33.9.0L160 255.9l-96.4 96.4c-9.4 9.4-24.6 9.4-33.9.0L7 329.7c-9.4-9.4-9.4-24.6.0-33.9l136-136c9.4-9.5 24.6-9.5 34-.1z"/></svg><script>let backToTopButton=document.getElementById("btt-button");window.onscroll=function(){scrollFunction()};function scrollFunction(){document.body.scrollTop>20||document.documentElement.scrollTop>20?backToTopButton.style.display="block":backToTopButton.style.display="none"}function topFunction(){smoothScrollToTop()}function smoothScrollToTop(){const e=()=>{const t=document.documentElement.scrollTop||document.body.scrollTop;t>0&&(window.requestAnimationFrame(e),window.scrollTo(0,t-t/8))};e()}</script></div></main><footer class=footer><span>&copy; 2025 Ren√© Filip</span>
<span>Made with &#10084;&#65039; using <a target=_blank href=https://github.com/526avijitgupta/gokarna>Gokarna</a></span></footer></body></html>